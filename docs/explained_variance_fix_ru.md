# Исправление расчёта explained variance в DistributionalPPO

## Контекст проблемы
При тюнинге гиперпараметров обучалась модель `DistributionalPPO`, и во время шага `train()` возникала ошибка `ValueError: operands could not be broadcast together with shapes (88,) (64,)`. Она появлялась внутри функции `safe_explained_variance`, когда массивы фактических вознаграждений и предсказанных значений имели разные длины. Из-за этого срывалось обучение и падал весь процесс Optuna.

Корень проблемы заключался в том, что при логировании метрик мы брали:
- таргеты (clamped returns) из мини-батчей буфера rollout;
- предсказания – только из последнего мини-батча, поскольку брались логиты, сохранённые политикой в ходе последнего прохода.

Когда размеры не совпадали (например, из-за неполного последнего батча или иного разбиения данных), NumPy не мог вычесть массивы и выбрасывал указанное исключение.

## Что было сделано

1. **Сбор данных по мини-батчам.** В цикле оптимизации теперь сохраняются обе величины — и обрезанные таргеты, и средние предсказания — для каждого мини-батча. Это гарантирует, что пары массивов образуются из одних и тех же выборок и имеют согласованные размеры.
2. **Безопасная сборка статистики.** После завершения эпох мы конкатенируем списки собранных тензоров и вычисляем explained variance уже по объединённым данным. Если по каким-то причинам списки пустые (например, при раннем выходе из-за early stopping), выполняется резервный путь: берём `returns` из rollout-буфера и предсказанные значения, восстановленные из последних логитов, но перед вычислением жёстко подрезаем массивы до общей длины.
3. **Дополнительная защита.** Перед вызовом `safe_explained_variance` проверяется совпадение размеров, а при несовпадении массивы приводятся к минимальной общей длине.

В итоге explained variance считается по согласованным данным, не выбрасывая исключения, и логирование метрик больше не ломает обучение.
