# –ò–º—è —Ñ–∞–π–ª–∞: train_model_multi_patch.py
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–§–ê–ó–ê 3 - CVaR):
# 1. –î–æ–±–∞–≤–ª–µ–Ω –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä `cvar_alpha` –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ Optuna.
# 2. `cvar_alpha` –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∞–≥–µ–Ω—Ç–∞ DistributionalPPO.
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–§–ê–ó–ê 6 - –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ HPO):
# 1. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —Å—Ä–µ–¥—ã (env_tr) —Ç–µ–ø–µ—Ä—å
#    —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ —Ñ–∞–π–ª –í–ù–ê–ß–ê–õ–ï –∏—Å–ø—ã—Ç–∞–Ω–∏—è.
# 2. –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ (env_va) –¥–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞ —Ç–µ–ø–µ—Ä—å —Å–æ–∑–¥–∞–µ—Ç—Å—è
#    –ø—É—Ç–µ–º –ó–ê–ì–†–£–ó–ö–ò —ç—Ç–∏—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫.
# 3. –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç —É—Ç–µ—á–∫—É –¥–∞–Ω–Ω—ã—Ö, –∫–æ–≥–¥–∞ env_va –≤—ã—á–∏—Å–ª—è–ª–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
#    –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏.
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–§–ê–ó–ê 7 - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ HPO –¥–ª—è CVaR):
# 1. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä `cvar_weight` –¥–æ–±–∞–≤–ª–µ–Ω –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ Optuna.
# 2. `cvar_weight` —Ç–µ–ø–µ—Ä—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –º–æ–¥–µ–ª–∏
#    DistributionalPPO, —á—Ç–æ –¥–µ–ª–∞–µ—Ç HPO –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º.
# –ò–ó–ú–ï–ù–ï–ù–ò–Ø (–û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò):
# 1. –î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ PyTorch –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –Ω–∞ GPU.
# 2. –î–æ–±–∞–≤–ª–µ–Ω–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ (`torch.compile`) –¥–ª—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è.
# 3. –¶–∏–∫–ª –æ—Ü–µ–Ω–∫–∏ –≤—ã–Ω–µ—Å–µ–Ω –≤ –±—ã—Å—Ç—Ä—ã–π Cython-–º–æ–¥—É–ª—å.

from __future__ import annotations

import os
import glob
import json
import pandas as pd
import numpy as np
from datetime import datetime
import shutil
import math
import argparse
import yaml
import sys
import hashlib
from functools import lru_cache
from collections.abc import Mapping, Sequence
from typing import Any, Callable
from features_pipeline import FeaturePipeline
from pathlib import Path

from core_config import (
    load_config,
    ExecutionProfile,
    load_timing_profiles,
    resolve_execution_timing,
    TrainConfig,
)

from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor, DummyVecEnv, VecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.vec_env import VecNormalize
import torch
import optuna
from optuna.samplers import TPESampler
from optuna.pruners import HyperbandPruner
from optuna.exceptions import TrialPruned
from torch.optim.lr_scheduler import OneCycleLR
import multiprocessing as mp
from leakguard import LeakGuard, LeakConfig
class AdversarialCallback(BaseCallback):
    """
    –ü—Ä–æ–≤–æ–¥–∏—Ç —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç—ã –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ä–µ–∂–∏–º–∞—Ö –∏ –°–û–•–†–ê–ù–Ø–ï–¢
    —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (Sortino Ratio) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞.
    """

    def __init__(
        self,
        eval_env: VecEnv,
        eval_freq: int,
        regimes: list,
        regime_duration: int,
        regime_config_path: str | None = None,
        liquidity_seasonality_path: str | None = None,
    ):
        super().__init__()
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.regimes = regimes
        self.regime_duration = regime_duration
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: {'regime_name': sortino_score}
        self.regime_metrics = {}

        if regime_config_path:
            os.environ["MARKET_REGIMES_JSON"] = regime_config_path
        self._liq_seasonality_path = liquidity_seasonality_path

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            print("\n--- Starting Adversarial Regime Stress Tests ---")
            
            for regime in self.regimes:
                print(f"Testing regime: {regime}...")
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ–∂–∏–º –≤ —Å—Ä–µ–¥–µ
                self.eval_env.env_method("set_market_regime", regime=regime, duration=self.regime_duration)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤ —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ
                _rewards, equity_curves = evaluate_policy_custom_cython(
                    self.model,
                    self.eval_env,
                    num_episodes=1 # –û–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π —ç–ø–∏–∑–æ–¥ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∂–∏–º–∞
                )
                
                # –°—á–∏—Ç–∞–µ–º Sortino –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                all_returns = [pd.Series(c).pct_change().dropna().to_numpy() for c in equity_curves if len(c) > 1]
                flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
                score = sortino_ratio(flat_returns)
                self.regime_metrics[regime] = score
                
                print(f"Regime '{regime}' | Sortino: {score:.4f}")

            # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
            self.eval_env.env_method("set_market_regime", regime='normal', duration=0)
            print("--- Adversarial Tests Finished ---\n")

            # Validate distributions against reference after stress tests
            try:
                dist_metrics, _ = compare_regime_distributions(
                    env=self.eval_env,
                    reference_path="configs/reference_regime_distributions.json",
                    n_steps=self.regime_duration,
                    tolerance=0.2,
                    raise_on_fail=False,
                )
                for r_name, vals in dist_metrics.items():
                    for m_name, val in vals.items():
                        if self.logger is not None:
                            self.logger.record(f"regime_dist/{r_name}/{m_name}", val)
            except Exception as exc:
                print(f"Distribution validation failed: {exc}")
        return True

    def get_regime_metrics(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–æ–≤."""
        return self.regime_metrics
from shared_memory_vec_env import SharedMemoryVecEnv


@lru_cache(maxsize=1)
def _get_distributional_ppo():
    from distributional_ppo import DistributionalPPO

    return DistributionalPPO

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í–∫–ª—é—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ PyTorch ---
# –ï—Å–ª–∏ GPU –∏ –≤–µ—Ä—Å–∏—è CUDA >= 11, –≤–∫–ª—é—á–∞–µ–º –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
if torch.cuda.is_available() and int(torch.version.cuda.split(".")[0]) >= 11:
    torch.set_float32_matmul_precision("high")
# –ü–æ–∑–≤–æ–ª—è–µ—Ç cuDNN –Ω–∞—Ö–æ–¥–∏—Ç—å –ª—É—á—à–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
torch.backends.cudnn.benchmark = True


from trading_patchnew import TradingEnv, DecisionTiming
from gymnasium import spaces
from wrappers.action_space import (
    DictToMultiDiscreteActionWrapper,
    LongOnlyActionWrapper,
)
from custom_policy_patch1 import CustomActorCriticPolicy
from fetch_all_data_patch import load_all_data
# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä—É—é Cython-—Ñ—É–Ω–∫—Ü–∏—é –æ—Ü–µ–Ω–∫–∏ ---
from evaluate_policy_custom_cython import evaluate_policy_custom_cython
from scripts.validate_regime_distributions import compare_regime_distributions
from data_validation import DataValidator
from utils.model_io import save_sidecar_metadata, check_model_compat
from watchdog_vec_env import WatchdogVecEnv
from scripts.offline_utils import load_offline_payload, resolve_split_bundle

# --- helper to compute SHA256 of liquidity seasonality file ---
def _file_sha256(path: str | None) -> str | None:
    if not path:
        return None
    try:
        with open(path, "rb") as f:
            return hashlib.sha256(f.read()).hexdigest()
    except FileNotFoundError:
        return None


def _wrap_action_space_if_needed(
    env,
    bins_vol: int = 101,
    *,
    action_overrides: dict[str, object] | None = None,
    long_only: bool = False,
):
    """
    If env.action_space is Dict with expected keys, wrap it into MultiDiscrete.
    Otherwise return as is.
    """

    wrapped_env = env
    try:
        if long_only:
            wrapped_env = LongOnlyActionWrapper(wrapped_env)
        if isinstance(wrapped_env.action_space, spaces.Dict):
            keys = set(getattr(wrapped_env.action_space, "spaces", {}).keys())
            expected = {"price_offset_ticks", "ttl_steps", "type", "volume_frac"}
            if expected.issubset(keys):
                return DictToMultiDiscreteActionWrapper(
                    wrapped_env,
                    bins_vol=bins_vol,
                    action_overrides=action_overrides,
                )
    except Exception:
        # If anything goes wrong, fail open (no wrapping)
        return wrapped_env
    return wrapped_env


def _coerce_timestamp(value) -> int | None:
    """Normalize timestamp representations to integer seconds."""

    if value is None:
        return None
    if isinstance(value, str):
        txt = value.strip()
        if not txt or txt.lower() == "none":
            return None
        try:
            value = int(txt)
        except ValueError:
            try:
                ts = pd.Timestamp(txt)
            except Exception as exc:  # pragma: no cover - defensive branch
                raise ValueError(f"Unable to parse timestamp string '{value}'.") from exc
            if ts.tzinfo is None:
                ts = ts.tz_localize("UTC")
            else:
                ts = ts.tz_convert("UTC")
            return int(ts.value // 10**9)
    if isinstance(value, pd.Timestamp):
        ts = value
        if ts.tzinfo is None:
            ts = ts.tz_localize("UTC")
        else:
            ts = ts.tz_convert("UTC")
        return int(ts.value // 10**9)
    if isinstance(value, (np.integer, int)):
        iv = int(value)
        if abs(iv) > 10_000_000_000:  # heuristic: values in ms
            return iv // 1000
        return iv
    if isinstance(value, (np.floating, float)):
        if math.isnan(value):
            return None
        return _coerce_timestamp(int(value))
    raise ValueError(f"Unsupported timestamp value: {value!r}")


def _normalize_interval(item) -> tuple[int | None, int | None]:
    if item is None:
        return (None, None)
    if isinstance(item, dict):
        start = item.get("start_ts")
        if start is None:
            start = item.get("start") or item.get("from")
        end = item.get("end_ts")
        if end is None:
            end = item.get("end") or item.get("to")
    elif isinstance(item, (list, tuple)) and len(item) == 2:
        start, end = item
    else:
        raise TypeError(f"Unsupported interval specification: {item!r}")
    start_ts = _coerce_timestamp(start)
    end_ts = _coerce_timestamp(end)
    if start_ts is not None and end_ts is not None and end_ts < start_ts:
        raise ValueError(f"Invalid interval with start {start_ts} after end {end_ts}")
    return (start_ts, end_ts)


def _extract_offline_split_overrides(
    payload: Mapping[str, Any] | None,
    dataset_key: str,
    *,
    fallback_split: str = "time",
) -> dict[str, list[dict[str, int | None]]]:
    """Extract inline time split overrides from an offline payload."""

    if payload is None:
        return {}

    datasets = payload.get("datasets") if isinstance(payload, Mapping) else None
    dataset_entry = datasets.get(dataset_key) if isinstance(datasets, Mapping) else None

    def _select_split_block(entry: Mapping[str, Any] | None) -> Mapping[str, Any] | None:
        if not isinstance(entry, Mapping):
            return None
        splits_block = entry.get("splits")
        if isinstance(splits_block, Mapping):
            return splits_block
        # Some payloads embed train/val/test directly at the dataset level.
        direct = {k: entry.get(k) for k in ("train", "val", "test") if entry.get(k) is not None}
        return direct or None

    def _pick_split_entry(block: Mapping[str, Any] | None) -> Mapping[str, Any] | None:
        if not isinstance(block, Mapping) or not block:
            return None
        if dataset_key in block and isinstance(block[dataset_key], Mapping):
            return block[dataset_key]
        if fallback_split in block and isinstance(block[fallback_split], Mapping):
            return block[fallback_split]
        # As a last resort take the first mapping value.
        for value in block.values():
            if isinstance(value, Mapping):
                return value
        return None

    split_entry = _pick_split_entry(_select_split_block(dataset_entry))
    if split_entry is None:
        # fall back to top-level split registry if provided
        split_entry = _pick_split_entry(
            payload.get("splits") if isinstance(payload, Mapping) else None
        )

    if not isinstance(split_entry, Mapping):
        return {}

    overrides: dict[str, list[dict[str, int | None]]] = {}
    for phase, entries in split_entry.items():
        if phase not in {"train", "val", "test"}:
            continue
        if isinstance(entries, (list, tuple)):
            iterable: Sequence[Any] = entries
        else:
            iterable = (entries,)
        normalized_items: list[dict[str, int | None]] = []
        for item in iterable:
            try:
                start_ts, end_ts = _normalize_interval(item)
            except Exception:
                continue
            normalized_items.append({"start_ts": start_ts, "end_ts": end_ts})
        if normalized_items:
            overrides[phase] = normalized_items
    return overrides


def _load_time_splits(data_cfg) -> tuple[str | None, dict[str, list[tuple[int | None, int | None]]]]:
    """Derive train/val/test time windows from config or an external manifest."""

    splits: dict[str, list[tuple[int | None, int | None]]] = {"train": [], "val": [], "test": []}
    version: str | None = getattr(data_cfg, "split_version", None)

    overrides = getattr(data_cfg, "split_overrides", None)
    if isinstance(overrides, Mapping):
        for phase, entries in overrides.items():
            if phase not in splits:
                splits[phase] = []
            if isinstance(entries, (list, tuple)):
                iterable = entries
            else:
                iterable = [entries]
            splits[phase].extend(_normalize_interval(item) for item in iterable)

    split_path = getattr(data_cfg, "split_path", None)
    if split_path:
        manifest_path = Path(split_path)
        if not manifest_path.exists():
            raise FileNotFoundError(f"Split manifest not found: {split_path}")
        with open(manifest_path, "r", encoding="utf-8") as fh:
            if manifest_path.suffix.lower() in (".yaml", ".yml"):
                raw = yaml.safe_load(fh) or {}
            else:
                raw = json.load(fh)
        version = raw.get("version") or raw.get("name") or version or manifest_path.stem
        raw_splits = raw.get("splits")
        if raw_splits is None:
            raw_splits = {k: raw.get(k) for k in ("train", "val", "test") if raw.get(k) is not None}
        if raw_splits is None:
            raise ValueError("Split manifest must contain 'splits' or per-phase keys (train/val/test).")
        for phase in ("train", "val", "test"):
            entries = raw_splits.get(phase)
            if entries is None:
                continue
            if isinstance(entries, (list, tuple)):
                iterable = entries
            else:
                iterable = [entries]
            splits[phase].extend(_normalize_interval(item) for item in iterable)

    for phase in ("train", "val", "test"):
        start_attr = getattr(data_cfg, f"{phase}_start_ts", None)
        end_attr = getattr(data_cfg, f"{phase}_end_ts", None)
        if start_attr is not None or end_attr is not None:
            splits[phase].append(_normalize_interval({"start_ts": start_attr, "end_ts": end_attr}))

    if not splits["train"]:
        fallback = _normalize_interval({"start_ts": getattr(data_cfg, "start_ts", None), "end_ts": getattr(data_cfg, "end_ts", None)})
        if fallback != (None, None):
            splits["train"].append(fallback)

    for phase, items in splits.items():
        cleaned = [item for item in items if not (item[0] is None and item[1] is None)]
        cleaned.sort(key=lambda it: it[0] if it[0] is not None else -float("inf"))
        splits[phase] = cleaned

    if not splits["train"]:
        raise ValueError("Training split is empty: provide train_start/train_end or a manifest")

    return version, splits


def _ensure_validation_split_present(
    dfs_with_roles: dict[str, pd.DataFrame],
    intervals: dict[str, list[tuple[int | None, int | None]]],
    timestamp_column: str,
    role_column: str,
) -> None:
    """Abort execution when the validation split yields zero rows."""

    val_rows = 0
    for df in dfs_with_roles.values():
        val_rows += int((df[role_column].astype(str) == "val").sum())
    if val_rows > 0:
        return

    configured = intervals.get("val", [])
    configured_desc = (
        ", ".join(_format_interval(it) for it in configured)
        if configured
        else "(not configured)"
    )
    observed_start, observed_end = _phase_bounds(dfs_with_roles, timestamp_column)
    coverage_desc = f"[{_fmt_ts(observed_start)} .. {_fmt_ts(observed_end)}]"

    msg_lines = [
        "Validation split is empty after applying configured intervals.",
        f"Configured validation intervals: {configured_desc}",
        f"Observed data coverage: {coverage_desc}",
    ]

    if configured:
        overlap_detected = None
        if observed_start is not None and observed_end is not None:
            overlap_detected = False
            for start, end in configured:
                start_cmp = observed_start if start is None else start
                end_cmp = observed_end if end is None else end
                if start_cmp <= observed_end and end_cmp >= observed_start:
                    overlap_detected = True
                    break
        if overlap_detected is False:
            msg_lines.append(
                "Configured validation window does not overlap with available data; "
                "regenerate or refresh the offline dataset."
            )
        else:
            msg_lines.append(
                "Adjust the validation split configuration or refresh the offline dataset "
                "to include the desired range."
            )
    else:
        msg_lines.append(
            "Provide validation split overrides or refresh the offline dataset to include validation data."
        )

    raise SystemExit("\n".join(msg_lines))


def _apply_role_column(
    df: pd.DataFrame,
    intervals: dict[str, list[tuple[int | None, int | None]]],
    timestamp_column: str,
    role_column: str,
) -> tuple[pd.DataFrame, bool]:
    """Annotate ``df`` with walk-forward roles using the provided intervals."""

    if timestamp_column not in df.columns:
        raise KeyError(f"DataFrame is missing timestamp column '{timestamp_column}'")
    ts = pd.to_numeric(df[timestamp_column], errors="coerce")
    roles = pd.Series(np.full(len(df), "none", dtype=object), index=df.index)

    for phase in ("train", "val", "test"):
        masks = []
        for start, end in intervals.get(phase, []):
            cur = pd.Series(True, index=df.index)
            if start is not None:
                cur &= ts >= start
            if end is not None:
                cur &= ts <= end
            masks.append(cur)
        if not masks:
            continue
        phase_mask = masks[0]
        for extra in masks[1:]:
            phase_mask |= extra
        assignable = (roles == "none") & phase_mask
        if assignable.any():
            roles.loc[assignable] = phase

    inferred_test = False
    if not intervals.get("test"):
        leftover = roles == "none"
        if leftover.any():
            roles.loc[leftover] = "test"
            inferred_test = True

    df_out = df.copy()
    df_out[role_column] = roles.values
    return df_out, inferred_test


def _phase_bounds(mapping: dict[str, pd.DataFrame], ts_col: str) -> tuple[int | None, int | None]:
    start: int | None = None
    end: int | None = None
    for df in mapping.values():
        ts = pd.to_numeric(df[ts_col], errors="coerce").dropna()
        if ts.empty:
            continue
        cur_start = int(ts.min())
        cur_end = int(ts.max())
        start = cur_start if start is None or cur_start < start else start
        end = cur_end if end is None or cur_end > end else end
    return start, end


def _fmt_ts(ts: int | None) -> str:
    if ts is None:
        return "None"
    return pd.to_datetime(int(ts), unit="s", utc=True).isoformat()


def _format_interval(interval: tuple[int | None, int | None]) -> str:
    start, end = interval
    return f"[{_fmt_ts(start)} .. {_fmt_ts(end)}]"

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò–ù–î–ò–ö–ê–¢–û–†–û–í (–ï–î–ò–ù–´–ô –ò–°–¢–û–ß–ù–ò–ö –ü–†–ê–í–î–´) ===
MA5_WINDOW = 5
MA20_WINDOW = 20
ATR_WINDOW = 14
RSI_WINDOW = 14
MACD_FAST = 12
MACD_SLOW = 26
MACD_SIGNAL = 9
MOMENTUM_WINDOW = 10
CCI_WINDOW = 14
BB_WINDOW = 20
OBV_MA_WINDOW = 50


class NanGuardCallback(BaseCallback):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç loss –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ NaN/Inf.
    –ü—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π Optuna-trial.
    """
    def __init__(self, threshold: float = float("inf"), verbose: int = 0):
        super().__init__(verbose)
        self.threshold = threshold       # –ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –ª–∏–º–∏—Ç ¬´—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π¬ª loss

    def _on_rollout_end(self) -> None:
        # 1) –ü—Ä–æ–≤–µ—Ä—è–µ–º loss, –µ—Å–ª–∏ SB3 –ø–æ–ª–æ–∂–∏–ª –µ–≥–æ –≤ –ª–æ–∫–∞–ª—ã
        last_loss = self.locals.get("loss", None)
        if last_loss is not None:
            if (not torch.isfinite(last_loss)) or (torch.abs(last_loss) > self.threshold):
                print("üö®  NaN/Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ loss  ‚Äî  –ø—Ä–µ—Ä—ã–≤–∞–µ–º trial")
                raise TrialPruned("NaN detected in loss")

        # 2) –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        for p in self.model.parameters():
            if p.grad is not None and (not torch.isfinite(p.grad).all()):
                print("üö®  NaN/Inf –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö  ‚Äî  –ø—Ä–µ—Ä—ã–≤–∞–µ–º trial")
                raise TrialPruned("NaN detected in gradients")

    def _on_step(self) -> bool:
        return True

class SortinoPruningCallback(BaseCallback):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è Optuna, –∫–æ—Ç–æ—Ä—ã–π –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç
    —Ä–µ—à–µ–Ω–∏–µ –æ –ø—Ä—É–Ω–∏–Ω–≥–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ö–û–≠–§–§–ò–¶–ò–ï–ù–¢–ê –°–û–†–¢–ò–ù–û, –∞ –Ω–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è.
    """
    def __init__(self, trial: optuna.Trial, eval_env: VecEnv, n_eval_episodes: int = 5, eval_freq: int = 10000, verbose: int = 0):
        super().__init__(verbose)
        self.trial = trial
        self.eval_env = eval_env
        self.n_eval_episodes = n_eval_episodes
        self.eval_freq = eval_freq

    def _on_step(self) -> bool:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤ –∑–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é Cython-—Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            rewards, equity_curves = evaluate_policy_custom_cython(
                self.model, 
                self.eval_env, 
                num_episodes=self.n_eval_episodes
            )
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Sortino –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –∫—Ä–∏–≤—ã—Ö –∫–∞–ø–∏—Ç–∞–ª–∞
            if not equity_curves:
                # –ï—Å–ª–∏ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ –æ—Ü–µ–Ω–∫–∞ –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å—á–∏—Ç–∞–µ–º Sortino —Ä–∞–≤–Ω—ã–º 0
                current_sortino = 0.0
            else:
                all_returns = [
                    pd.Series(curve).pct_change().dropna().to_numpy() 
                    for curve in equity_curves if len(curve) > 1
                ]
                flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
                current_sortino = sortino_ratio(flat_returns)

            if self.verbose > 0:
                print(f"Step {self.n_calls}: Pruning check with Sortino Ratio = {current_sortino:.4f}")

            if self.logger is not None:
                self.logger.record("pruning/sortino_ratio", current_sortino)

            # 1. –°–æ–æ–±—â–∞–µ–º Optuna –æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ (—Ç–µ–ø–µ—Ä—å —ç—Ç–æ Sortino)
            self.trial.report(current_sortino, self.n_calls)

            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –Ω—É–∂–Ω–æ –ª–∏ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —ç—Ç–æ—Ç trial
            if self.trial.should_prune():
                raise TrialPruned(f"Trial pruned at step {self.n_calls} with Sortino Ratio: {current_sortino:.4f}")

        return True

class ObjectiveScorePruningCallback(BaseCallback):
    """
    Callback –¥–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –ø–æ–ª–Ω—É—é –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É objective_score.
    –†–∞–±–æ—Ç–∞–µ—Ç —Ä–µ–∂–µ, —á–µ–º SortinoPruningCallback, —Ç–∞–∫ –∫–∞–∫ –æ—Ü–µ–Ω–∫–∞ –∑–∞–Ω–∏–º–∞–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏.
    """
    def __init__(self, trial: optuna.Trial, eval_env: VecEnv, eval_freq: int = 40000, verbose: int = 0):
        super().__init__(verbose)
        self.trial = trial
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        
        # –í–µ—Å–∞, –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ
        self.main_weight = 0.5
        self.choppy_weight = 0.3
        self.trend_weight = 0.2
        self.regime_duration = 2_500

    def _on_step(self) -> bool:
        if self.eval_freq > 0 and self.n_calls > 0 and self.n_calls % self.eval_freq == 0:
            
            print(f"\n--- Step {self.n_calls}: Starting comprehensive pruning check with Objective Score ---")
            
            regimes_to_evaluate = ['normal', 'choppy_flat', 'strong_trend']
            evaluated_metrics = {}

            try:
                for regime in regimes_to_evaluate:
                    if self.verbose > 0:
                        print(f"Pruning evaluation: testing regime '{regime}'...")

                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–¥–≤–µ—Ä—Å–∞—Ä–∏–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
                    if regime != 'normal':
                        self.eval_env.env_method("set_market_regime", regime=regime, duration=self.regime_duration)

                    # –î–ª—è –ø—Ä—É–Ω–∏–Ω–≥–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ–Ω—å—à–µ–≥–æ —á–∏—Å–ª–∞ —ç–ø–∏–∑–æ–¥–æ–≤, —á–µ–º –≤ —Ñ–∏–Ω–∞–ª–µ
                    num_episodes = 5  # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 5 —ç–ø–∏–∑–æ–¥–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
                    
                    _rewards, equity_curves = evaluate_policy_custom_cython(
                        self.model, self.eval_env, num_episodes=num_episodes
                    )
                    
                    all_returns = [pd.Series(c).pct_change().dropna().to_numpy() for c in equity_curves if len(c) > 1]
                    flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
                    score = sortino_ratio(flat_returns)
                    evaluated_metrics[regime] = score

            finally:
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –≤—Å–µ–≥–¥–∞ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ä–µ–¥—É –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º,
                # —á—Ç–æ–±—ã –Ω–µ –≤–ª–∏—è—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –¥—Ä—É–≥–∏–µ –∫–æ–ª–±—ç–∫–∏.
                self.eval_env.env_method("set_market_regime", regime='normal', duration=0)

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É
            main_sortino = evaluated_metrics.get('normal', -1.0)
            choppy_score = evaluated_metrics.get('choppy_flat', -1.0)
            trend_score = evaluated_metrics.get('strong_trend', -1.0)
            
            objective_score = (self.main_weight * main_sortino + 
                               self.choppy_weight * choppy_score + 
                               self.trend_weight * trend_score)

            if self.verbose > 0:
                print(f"Comprehensive pruning check complete. Objective Score: {objective_score:.4f}")
                print(f"Components -> Main: {main_sortino:.4f}, Choppy: {choppy_score:.4f}, Trend: {trend_score:.4f}\n")

            if self.logger is not None:
                self.logger.record("pruning/objective_score", objective_score)
                self.logger.record("pruning/objective_main", main_sortino)
                self.logger.record("pruning/objective_choppy", choppy_score)
                self.logger.record("pruning/objective_trend", trend_score)

            # –°–æ–æ–±—â–∞–µ–º Optuna –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø—Ä—É–Ω–∏–Ω–≥–∞
            self.trial.report(objective_score, self.n_calls)
            if self.trial.should_prune():
                raise TrialPruned(f"Trial pruned at step {self.n_calls} with Objective Score: {objective_score:.4f}")

        return True

def sharpe_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    std = np.std(returns)
    return np.mean(returns - risk_free_rate) / (std + 1e-9) * np.sqrt(365 * 24)

def sortino_ratio(returns: np.ndarray, risk_free_rate: float = 0.0) -> float:
    downside = returns[returns < risk_free_rate] - risk_free_rate
    if downside.size == 0:
        # –ï—Å–ª–∏ –Ω–µ—Ç —É–±—ã—Ç–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ (–∫–∞–∫ –≤ –®–∞—Ä–ø–µ).
        # –≠—Ç–æ –±–æ–ª–µ–µ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∏—Å–∫, —á–µ–º –≤–æ–∑–≤—Ä–∞—Ç –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã.
        std = np.std(returns)
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å, –µ—Å–ª–∏ –≤—Å–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã.
        if std < 1e-9:
            return 0.0
        return np.mean(returns - risk_free_rate) / std * np.sqrt(365 * 24)

    downside_std = np.sqrt(np.mean(downside**2)) + 1e-9
    # –≠—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π, –µ—Å–ª–∏ downside_std –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑,
    # –Ω–æ –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏.
    return np.mean(returns - risk_free_rate) / downside_std * np.sqrt(365 * 24)

# --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –°—Ç–∞—Ä–∞—è Python-—Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞, —Ç–∞–∫ –∫–∞–∫ –∑–∞–º–µ–Ω–µ–Ω–∞ –Ω–∞ Cython-–≤–µ—Ä—Å–∏—é ---

def objective(trial: optuna.Trial,
              cfg: TrainConfig,
              total_timesteps: int,
              train_data_by_token: dict,
              train_obs_by_token: dict,
              val_data_by_token: dict,
              val_obs_by_token: dict,
              test_data_by_token: dict,
              test_obs_by_token: dict,
              norm_stats: dict,
              sim_config: dict,
              timing_env_kwargs: dict,
              leak_guard_kwargs: dict,
              trials_dir: Path,
              tensorboard_log_dir: Path | None):

    print(f">>> Trial {trial.number+1} with budget={total_timesteps}")

    def _extract_bins_vol_from_cfg(cfg, default=101):
        try:
            aw = getattr(getattr(cfg, "algo", None), "action_wrapper", None)
            val = getattr(aw, "bins_vol", None) if aw is not None else None
            if val is None and hasattr(aw, "__dict__"):
                val = aw.__dict__.get("bins_vol")
            return max(2, int(val)) if val is not None else int(default)
        except Exception:
            return int(default)

    bins_vol = _extract_bins_vol_from_cfg(cfg, default=101)

    def _resolve_nested(cfg_obj, attr: str):
        if cfg_obj is None:
            return None
        if isinstance(cfg_obj, Mapping):
            return cfg_obj.get(attr)
        try:
            value = getattr(cfg_obj, attr)
        except AttributeError:
            value = None
        if value is not None:
            return value
        for extra_name in ("__dict__", "__pydantic_extra__", "model_extra"):
            try:
                extra = getattr(cfg_obj, extra_name)
            except AttributeError:
                extra = None
            if isinstance(extra, Mapping) and attr in extra:
                return extra.get(attr)
        return None

    def _coerce_bool(value: object) -> bool:
        if isinstance(value, str):
            return value.strip().lower() in {"1", "true", "yes", "y", "on"}
        return bool(value)

    def _extract_action_overrides_from_cfg(cfg_obj) -> tuple[dict[str, object], bool]:
        algo_cfg = _resolve_nested(cfg_obj, "algo")
        actions_cfg = _resolve_nested(algo_cfg, "actions")
        if actions_cfg is None:
            return {}, False
        if hasattr(actions_cfg, "dict"):
            try:
                actions_payload = actions_cfg.dict()
            except TypeError:
                actions_payload = None
        else:
            actions_payload = None
        if actions_payload is None:
            if isinstance(actions_cfg, Mapping):
                actions_payload = dict(actions_cfg)
            else:
                payload = {}
                for extra_name in ("__dict__", "__pydantic_extra__", "model_extra"):
                    try:
                        extra = getattr(actions_cfg, extra_name)
                    except AttributeError:
                        extra = None
                    if isinstance(extra, Mapping):
                        payload.update(extra)
                actions_payload = payload
        if not isinstance(actions_payload, Mapping):
            return {}, False
        overrides: dict[str, object] = {}
        long_only_flag = False
        if "lock_price_offset" in actions_payload:
            overrides["lock_price_offset"] = _coerce_bool(
                actions_payload.get("lock_price_offset")
            )
        if "lock_ttl" in actions_payload:
            overrides["lock_ttl"] = _coerce_bool(actions_payload.get("lock_ttl"))
        if "fixed_type" in actions_payload:
            overrides["fixed_type"] = actions_payload.get("fixed_type")
        if "long_only" in actions_payload:
            long_only_flag = _coerce_bool(actions_payload.get("long_only"))
        return overrides, long_only_flag

    action_overrides, long_only_flag = _extract_action_overrides_from_cfg(cfg)


    # –ò–°–ü–†–ê–í–õ–ï–ù–û: window_size –≤–æ–∑–≤—Ä–∞—â–µ–Ω –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ HPO
    def _get_model_param_value(cfg: TrainConfig, key: str):
        try:
            model_cfg = getattr(cfg, "model", None)
            if model_cfg is None:
                return None
            params_obj = getattr(model_cfg, "params", None)
            if params_obj is None:
                return None
            if isinstance(params_obj, Mapping):
                return params_obj.get(key)
            getter = getattr(params_obj, "get", None)
            if callable(getter):
                try:
                    return getter(key)
                except Exception:
                    pass
            if hasattr(params_obj, key):
                return getattr(params_obj, key)
            if hasattr(params_obj, "__dict__"):
                if key in params_obj.__dict__:
                    return params_obj.__dict__.get(key)
            dict_method = getattr(params_obj, "dict", None)
            if callable(dict_method):
                try:
                    params_dict = dict_method()
                except TypeError:
                    params_dict = None
                if isinstance(params_dict, Mapping):
                    return params_dict.get(key)
        except Exception:
            return None
        return None

    def _coerce_optional_int(value, key: str):
        if value is None:
            return None
        if isinstance(value, bool):
            raise ValueError(
                f"Invalid value '{value}' for '{key}' (expected integer) in cfg.model.params"
            )
        if isinstance(value, int):
            return int(value)
        try:
            coerced = int(value)
        except (TypeError, ValueError):
            raise ValueError(
                f"Invalid value '{value}' for '{key}' (expected integer) in cfg.model.params"
            )
        return coerced

    def _coerce_optional_float(value, key: str):
        if value is None:
            return None
        if isinstance(value, (int, float)):
            return float(value)
        try:
            return float(value)
        except (TypeError, ValueError):
            raise ValueError(
                f"Invalid value '{value}' for '{key}' (expected float-compatible) in cfg.model.params"
            )

    learning_rate_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "learning_rate"), "learning_rate"
    )
    gamma_cfg = _coerce_optional_float(_get_model_param_value(cfg, "gamma"), "gamma")
    gae_lambda_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "gae_lambda"), "gae_lambda"
    )
    clip_range_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "clip_range"), "clip_range"
    )
    ent_coef_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "ent_coef"), "ent_coef"
    )
    vf_coef_cfg = _coerce_optional_float(_get_model_param_value(cfg, "vf_coef"), "vf_coef")
    max_grad_norm_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "max_grad_norm"), "max_grad_norm"
    )
    n_steps_cfg = _coerce_optional_int(_get_model_param_value(cfg, "n_steps"), "n_steps")
    batch_size_cfg = _coerce_optional_int(
        _get_model_param_value(cfg, "batch_size"), "batch_size"
    )

    trade_frequency_penalty_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "trade_frequency_penalty"),
        "trade_frequency_penalty",
    )
    turnover_penalty_coef_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "turnover_penalty_coef"),
        "turnover_penalty_coef",
    )

    v_range_ema_alpha_cfg = _coerce_optional_float(
        _get_model_param_value(cfg, "v_range_ema_alpha"), "v_range_ema_alpha"
    )

    params = {
        "window_size": trial.suggest_categorical("window_size", [10, 20, 30]),
        "n_steps": n_steps_cfg if n_steps_cfg is not None else trial.suggest_categorical("n_steps", [512, 1024, 2048]),
        "n_epochs": trial.suggest_int("n_epochs", 1, 5),
        "batch_size": batch_size_cfg if batch_size_cfg is not None else trial.suggest_categorical("batch_size", [64, 128, 256]),
        "ent_coef": ent_coef_cfg if ent_coef_cfg is not None else trial.suggest_float("ent_coef", 5e-5, 5e-3, log=True),
        "learning_rate": learning_rate_cfg if learning_rate_cfg is not None else trial.suggest_float("learning_rate", 1e-4, 5e-4, log=True),
        "risk_aversion_drawdown": trial.suggest_float("risk_aversion_drawdown", 0.05, 0.3),
        "risk_aversion_variance": trial.suggest_float("risk_aversion_variance", 0.005, 0.01),
        "weight_decay": trial.suggest_float("weight_decay", 1e-6, 1e-2, log=True),
        "gamma": gamma_cfg if gamma_cfg is not None else trial.suggest_float("gamma", 0.97, 0.995),
        "gae_lambda": gae_lambda_cfg if gae_lambda_cfg is not None else trial.suggest_float("gae_lambda", 0.8, 1.0),
        "clip_range": clip_range_cfg if clip_range_cfg is not None else trial.suggest_float("clip_range", 0.12, 0.18),
        "max_grad_norm": max_grad_norm_cfg if max_grad_norm_cfg is not None else trial.suggest_float("max_grad_norm", 0.3, 1.0),
        "hidden_dim": trial.suggest_categorical("hidden_dim", [64, 128, 256]),
        "atr_multiplier": trial.suggest_float("atr_multiplier", 1.5, 3.0),
        "trailing_atr_mult": trial.suggest_float("trailing_atr_mult", 1.0, 2.0),
        "tp_atr_mult": trial.suggest_float("tp_atr_mult", 2.0, 4.0),
        "momentum_factor": trial.suggest_float("momentum_factor", 0.1, 0.7),
        "mean_reversion_factor": trial.suggest_float("mean_reversion_factor", 0.2, 0.8),
        "adversarial_factor": trial.suggest_float("adversarial_factor", 0.3, 0.9),
        "vf_coef": vf_coef_cfg if vf_coef_cfg is not None else trial.suggest_float("vf_coef", 0.05, 0.5, log=True), # <-- –î–û–ë–ê–í–õ–ï–ù–û
        "v_range_ema_alpha": v_range_ema_alpha_cfg if v_range_ema_alpha_cfg is not None else trial.suggest_float("v_range_ema_alpha", 0.005, 0.05, log=True),
    }

    if trade_frequency_penalty_cfg is not None:
        params["trade_frequency_penalty"] = trade_frequency_penalty_cfg
    else:
        params["trade_frequency_penalty"] = trial.suggest_float(
            "trade_frequency_penalty", 1e-5, 5e-4, log=True
        )

    if turnover_penalty_coef_cfg is not None:
        params["turnover_penalty_coef"] = turnover_penalty_coef_cfg
    else:
        params["turnover_penalty_coef"] = trial.suggest_float(
            "turnover_penalty_coef", 0.0, 5e-4
        )

    cql_alpha_cfg = _coerce_optional_float(_get_model_param_value(cfg, "cql_alpha"), "cql_alpha")
    if cql_alpha_cfg is not None:
        params["cql_alpha"] = cql_alpha_cfg
    else:
        params["cql_alpha"] = trial.suggest_float("cql_alpha", 0.1, 10.0, log=True)

    cql_beta_cfg = _coerce_optional_float(_get_model_param_value(cfg, "cql_beta"), "cql_beta")
    if cql_beta_cfg is not None:
        params["cql_beta"] = cql_beta_cfg
    else:
        params["cql_beta"] = trial.suggest_float("cql_beta", 1.0, 10.0)

    cvar_alpha_cfg = _coerce_optional_float(_get_model_param_value(cfg, "cvar_alpha"), "cvar_alpha")
    if cvar_alpha_cfg is not None:
        params["cvar_alpha"] = cvar_alpha_cfg
    else:
        params["cvar_alpha"] = trial.suggest_float("cvar_alpha", 0.01, 0.20)

    cvar_weight_cfg = _coerce_optional_float(_get_model_param_value(cfg, "cvar_weight"), "cvar_weight")
    if cvar_weight_cfg is not None:
        params["cvar_weight"] = cvar_weight_cfg
    else:
        params["cvar_weight"] = trial.suggest_float("cvar_weight", 0.1, 2.0, log=True)

    cvar_cap_cfg = _coerce_optional_float(_get_model_param_value(cfg, "cvar_cap"), "cvar_cap")
    if cvar_cap_cfg is not None:
        params["cvar_cap"] = cvar_cap_cfg
    else:
        params["cvar_cap"] = trial.suggest_float("cvar_cap", 0.01, 1.0, log=True)
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫–Ω–æ —Å–∞–º–æ–≥–æ "–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ" –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    #    –≠—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ C++ —Å–∏–º—É–ª—è—Ç–æ—Ä.
    #    (–í –¥–∞–Ω–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –æ–Ω–∏ –∂–µ—Å—Ç–∫–æ –∑–∞–¥–∞–Ω—ã –≤ TradingEnv, –Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –±–µ—Ä–µ–º –∏—Ö –∏–∑ HPO)
    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–∫–Ω–æ —Å–∞–º–æ–≥–æ "–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ" –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç.
    #    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —Å—Ä–µ–¥—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
    slowest_window = max(
        params["window_size"],
        MA20_WINDOW,
        MACD_SLOW,
        ATR_WINDOW,
        RSI_WINDOW,
        CCI_WINDOW,
        BB_WINDOW,
        OBV_MA_WINDOW
    )
    
    # 2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–∏–æ–¥ –ø—Ä–æ–≥—Ä–µ–≤–∞ —Å –∑–∞–ø–∞—Å–æ–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è x2).
    #    –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –¥–∞–∂–µ –¥–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–π,
    #    –∫–∞–∫ –≤ —Å–∏–≥–Ω–∞–ª—å–Ω–æ–π –ª–∏–Ω–∏–∏ MACD.
    warmup_period = slowest_window * 2

    num_atoms_cfg = _coerce_optional_int(_get_model_param_value(cfg, "num_atoms"), "num_atoms")
    v_min_cfg = _coerce_optional_float(_get_model_param_value(cfg, "v_min"), "v_min")
    v_max_cfg = _coerce_optional_float(_get_model_param_value(cfg, "v_max"), "v_max")

    num_atoms = num_atoms_cfg if num_atoms_cfg is not None else 51
    v_min = v_min_cfg if v_min_cfg is not None else -1.0
    v_max = v_max_cfg if v_max_cfg is not None else 1.0

    if num_atoms < 1:
        raise ValueError("Invalid configuration: 'num_atoms' must be >= 1 in cfg.model.params")
    if v_max <= v_min:
        raise ValueError(
            "Invalid configuration: 'v_max' must be greater than 'v_min' in cfg.model.params"
        )

    print(f"[cfg override] value head: num_atoms={num_atoms}, v_min={v_min}, v_max={v_max}")

    policy_arch_params = {
        "hidden_dim": params["hidden_dim"],
        "use_memory": True,
        "num_atoms": num_atoms,
        "v_min": v_min,
        "v_max": v_max,
    }

    if not train_data_by_token: raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —ç—Ç–æ–º trial.")

    n_envs = 8
    print(f"–ó–∞–ø—É—Å–∫–∞–µ–º {n_envs} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥...")

    total_batch_size = params["n_steps"] * n_envs
    batch_size = params["batch_size"]
    if batch_size <= 0:
        raise ValueError("Invalid configuration: 'batch_size' must be a positive integer in cfg.model.params")
    if batch_size > total_batch_size:
        raise ValueError(
            "Invalid configuration: 'batch_size' cannot exceed n_steps * num_envs in cfg.model.params"
        )
    if total_batch_size % batch_size != 0:
        raise ValueError(
            "Invalid configuration: 'batch_size' must evenly divide n_steps * num_envs in cfg.model.params"
        )
    leak_guard_train = LeakGuard(LeakConfig(**leak_guard_kwargs))
    leak_guard_val = LeakGuard(LeakConfig(**leak_guard_kwargs))

    train_symbol_items = sorted(train_data_by_token.items())
    if not train_symbol_items:
        raise ValueError("–ù–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥.")

    def make_env_train(rank: int):
        symbol_idx = rank % len(train_symbol_items)
        symbol, df = train_symbol_items[symbol_idx]

        def _init():
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π seed –¥–ª—è –∫–∞–∂–¥–æ–≥–æ trial-–∞ –∏ –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞
            # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ –¥–µ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é
            unique_seed = trial.number * 100 + rank
            env_params = {
                # 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–±–∏—Ä–∞–µ—Ç Optuna
                "risk_aversion_drawdown": params["risk_aversion_drawdown"],
                "risk_aversion_variance": params["risk_aversion_variance"],
                "atr_multiplier": params["atr_multiplier"],
                "trailing_atr_mult": params["trailing_atr_mult"],
                "tp_atr_mult": params["tp_atr_mult"],
                "window_size": params.get("window_size", 20),
                "gamma": params["gamma"],
                "trade_frequency_penalty": params["trade_frequency_penalty"],
                "turnover_penalty_coef": params["turnover_penalty_coef"],
                "momentum_factor": params["momentum_factor"],
                "mean_reversion_factor": params["mean_reversion_factor"],
                "adversarial_factor": params["adversarial_factor"],

                # 2. –î–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏—é objective
                "norm_stats": norm_stats,

                # 3. –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                "mode": "train",
                "reward_shaping": False,
                "warmup_period": warmup_period,
                "ma5_window": MA5_WINDOW,
                "ma20_window": MA20_WINDOW,
                "atr_window": ATR_WINDOW,
                "rsi_window": RSI_WINDOW,
                "macd_fast": MACD_FAST,
                "macd_slow": MACD_SLOW,
                "macd_signal": MACD_SIGNAL,
                "momentum_window": MOMENTUM_WINDOW,
                "cci_window": CCI_WINDOW,
                "bb_window": BB_WINDOW,
                "obv_ma_window": OBV_MA_WINDOW,

            }
            env_params.update(sim_config)
            env_params.update(timing_env_kwargs)

            # –°–æ–∑–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Å—Ä–µ–¥—ã
            env = TradingEnv(
                df,
                **env_params,
                leak_guard=leak_guard_train,
                seed=unique_seed,
            )
            setattr(env, "selected_symbol", symbol)
            env = _wrap_action_space_if_needed(
                env,
                bins_vol=bins_vol,
                action_overrides=action_overrides,
                long_only=long_only_flag,
            )
            return env
        return _init

    env_constructors = [make_env_train(rank=i) for i in range(n_envs)]
    train_stats_path = trials_dir / f"vec_normalize_train_{trial.number}.pkl"
    os.makedirs(trials_dir, exist_ok=True)

    base_env_tr = WatchdogVecEnv(env_constructors)
    monitored_env_tr = VecMonitor(base_env_tr)
    env_tr = VecNormalize(
        monitored_env_tr,
        norm_obs=False,
        norm_reward=False,
        clip_reward=None,
        gamma=params["gamma"],
    )

    env_tr.save(str(train_stats_path))
    save_sidecar_metadata(str(train_stats_path), extra={"kind": "vecnorm_stats", "phase": "train"})

    val_symbol_items = sorted(val_data_by_token.items())
    if not val_symbol_items:
        raise ValueError("–ù–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥.")

    def _make_val_env_factory(symbol: str, df: pd.DataFrame):
        env_val_params = {
            "norm_stats": norm_stats,
            "window_size": params["window_size"],
            "gamma": params["gamma"],
            "atr_multiplier": params["atr_multiplier"],
            "trailing_atr_mult": params["trailing_atr_mult"],
            "tp_atr_mult": params["tp_atr_mult"],
            "trade_frequency_penalty": params["trade_frequency_penalty"],
            "turnover_penalty_coef": params["turnover_penalty_coef"],
            "mode": "val",
            "reward_shaping": False,
            "warmup_period": warmup_period,
            "ma5_window": MA5_WINDOW,
            "ma20_window": MA20_WINDOW,
            "atr_window": ATR_WINDOW,
            "rsi_window": RSI_WINDOW,
            "macd_fast": MACD_FAST,
            "macd_slow": MACD_SLOW,
            "macd_signal": MACD_SIGNAL,
            "momentum_window": MOMENTUM_WINDOW,
            "cci_window": CCI_WINDOW,
            "bb_window": BB_WINDOW,
            "obv_ma_window": OBV_MA_WINDOW
        }
        env_val_params.update(sim_config)
        env_val_params.update(timing_env_kwargs)
        env = TradingEnv(
            df,
            **env_val_params,
            leak_guard=leak_guard_val,
        )
        setattr(env, "selected_symbol", symbol)
        env = _wrap_action_space_if_needed(
            env,
            bins_vol=bins_vol,
            action_overrides=action_overrides,
            long_only=long_only_flag,
        )
        return env

    val_env_fns = [
        lambda symbol=symbol, df=df: _make_val_env_factory(symbol, df)
        for symbol, df in val_symbol_items
    ]
    monitored_env_va = VecMonitor(DummyVecEnv(val_env_fns))
    check_model_compat(str(train_stats_path))
    env_va = VecNormalize.load(str(train_stats_path), monitored_env_va)
    env_va.training = False
    env_va.norm_reward = False
    env_va.clip_reward = None

    val_stats_path = trials_dir / f"vec_normalize_val_{trial.number}.pkl"
    env_va.save(str(val_stats_path))
    save_sidecar_metadata(str(val_stats_path), extra={"kind": "vecnorm_stats", "phase": "val"})

    policy_kwargs = {
        "arch_params": policy_arch_params,
        "optimizer_class": torch.optim.AdamW,
        "optimizer_kwargs": {"weight_decay": params["weight_decay"]},
    }
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±—É–¥–µ—Ç —Å–æ–±—Ä–∞–Ω –ø–æ–ª–Ω—ã–π –±—É—Ñ–µ—Ä –¥–∞–Ω–Ω—ã—Ö (rollout)
    num_rollouts = math.ceil(total_timesteps / (params["n_steps"] * n_envs))
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º, –Ω–∞ —Å–∫–æ–ª—å–∫–æ –º–∏–Ω–∏-–±–∞—Ç—á–µ–π –¥–µ–ª–∏—Ç—Å—è –∫–∞–∂–¥—ã–π —Ä–æ–ª–ª–∞—É—Ç
    num_minibatches_per_rollout = total_batch_size // batch_size
    
    # –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∑–∞ –≤—Å–µ –æ–±—É—á–µ–Ω–∏–µ
    total_optimizer_steps = num_rollouts * params["n_epochs"] * num_minibatches_per_rollout

    optimization_cfg = getattr(cfg, "optimization", None)
    scheduler_cfg = None
    if isinstance(optimization_cfg, Mapping):
        scheduler_cfg = optimization_cfg.get("scheduler")
    elif optimization_cfg is not None:
        scheduler_cfg = getattr(optimization_cfg, "scheduler", None)

    scheduler_enabled_value = True
    if isinstance(scheduler_cfg, Mapping):
        scheduler_enabled_value = scheduler_cfg.get("enabled", True)
    elif scheduler_cfg is not None:
        scheduler_enabled_value = getattr(scheduler_cfg, "enabled", True)

    if isinstance(scheduler_enabled_value, str):
        scheduler_enabled = scheduler_enabled_value.strip().lower() not in {"0", "false", "no", "off"}
    elif scheduler_enabled_value is None:
        scheduler_enabled = True
    else:
        scheduler_enabled = bool(scheduler_enabled_value)

    if scheduler_enabled:
        # –°–æ–∑–¥–∞–µ–º lambda-—Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
        # SB3 –≤—ã–∑–æ–≤–µ—Ç –µ–µ —Å–æ —Å–≤–æ–∏–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–º, –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º
        def scheduler_fn(optimizer):
            return OneCycleLR(
                optimizer=optimizer,
                max_lr=params["learning_rate"] * 3,
                total_steps=total_optimizer_steps,
            )

        # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –µ–µ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ policy_kwargs
        policy_kwargs["optimizer_scheduler_fn"] = scheduler_fn
    DistributionalPPO = _get_distributional_ppo()

    use_torch_compile_cfg = _get_model_param_value(cfg, "use_torch_compile")
    if isinstance(use_torch_compile_cfg, str):
        use_torch_compile = use_torch_compile_cfg.strip().lower() in {"1", "true", "yes", "on"}
    elif use_torch_compile_cfg is None:
        use_torch_compile = False
    else:
        use_torch_compile = bool(use_torch_compile_cfg)

    tb_log_path: Path | None = None
    if tensorboard_log_dir is not None:
        tb_log_path = tensorboard_log_dir / f"trial_{trial.number:03d}"
        tb_log_path.mkdir(parents=True, exist_ok=True)

    model = DistributionalPPO(
        use_torch_compile=use_torch_compile,
        v_range_ema_alpha=params["v_range_ema_alpha"],
        policy=CustomActorCriticPolicy,
        env=env_tr,
        cql_alpha=params["cql_alpha"],
        cql_beta=params["cql_beta"],
        cvar_alpha=params["cvar_alpha"],
        vf_coef=params["vf_coef"],
        cvar_weight=params["cvar_weight"],
        cvar_cap=params["cvar_cap"],
        
        learning_rate=params["learning_rate"],
        n_steps=params["n_steps"],
        n_epochs=params["n_epochs"],
        batch_size=params["batch_size"],
        ent_coef=params["ent_coef"],
        gamma=params["gamma"],
        gae_lambda=params["gae_lambda"],
        clip_range=params["clip_range"],
        max_grad_norm=params["max_grad_norm"],
        policy_kwargs=policy_kwargs,
        tensorboard_log=str(tb_log_path) if tb_log_path is not None else None,
        verbose=1
    )

    



    nan_guard = NanGuardCallback()

    # –ë—ã—Å—Ç—Ä—ã–π –∫–æ–ª–±—ç–∫ –¥–ª—è —Ä–∞–Ω–Ω–µ–≥–æ –æ—Ç—Å–µ—á–µ–Ω–∏—è –ø–æ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç—Ä–∏–∫–µ
    sortino_pruner = SortinoPruningCallback(trial, eval_env=env_va, eval_freq=8_000, n_eval_episodes=10)

    # –ú–µ–¥–ª–µ–Ω–Ω—ã–π, –Ω–æ —Ç–æ—á–Ω—ã–π –∫–æ–ª–±—ç–∫ –¥–ª—è –ø–æ–∑–¥–Ω–µ–≥–æ –æ—Ç—Å–µ—á–µ–Ω–∏—è –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ
    objective_pruner = ObjectiveScorePruningCallback(trial, eval_env=env_va, eval_freq=40_000, verbose=1)

    all_callbacks = [nan_guard, sortino_pruner, objective_pruner]

    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=all_callbacks,
            progress_bar=True,
            tb_log_name=f"trial_{trial.number:03d}" if tb_log_path is not None else "run"
        )
    finally:
        env_tr.close()
        env_va.close()

    trial_model_path = trials_dir / f"trial_{trial.number}_model.zip"
    model.save(str(trial_model_path))
    save_sidecar_metadata(str(trial_model_path), extra={"kind": "sb3_model", "trial": int(trial.number)})

    

    print(f"<<< Trial {trial.number+1} finished training, starting unified final evaluation‚Ä¶")

    eval_phase_data = test_data_by_token if test_data_by_token else val_data_by_token
    eval_phase_obs = test_obs_by_token if test_data_by_token else val_obs_by_token
    eval_phase_name = "test" if test_data_by_token else "val"
    if not eval_phase_data:
        raise ValueError("No data available for validation/test evaluation. Check time split configuration.")

    # 1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ —Ä–µ–∂–∏–º—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    regimes_to_evaluate = ['normal', 'choppy_flat', 'strong_trend']
    final_metrics = {}
    regime_duration = 2_500 

    # 2. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –≤ –∫–∞–∂–¥–æ–º —Ä–µ–∂–∏–º–µ
    for regime in regimes_to_evaluate:
        symbol_equity_curves: list[list[float]] = []
        test_stats_path = trials_dir / f"vec_normalize_test_{trial.number}.pkl"

        for symbol, df in sorted(eval_phase_data.items()):
            def make_final_eval_env(symbol: str = symbol, df: pd.DataFrame = df):
                final_env_params = {
                    "norm_stats": norm_stats, "window_size": params["window_size"],
                    "gamma": params["gamma"], "atr_multiplier": params["atr_multiplier"],
                    "trailing_atr_mult": params["trailing_atr_mult"], "tp_atr_mult": params["tp_atr_mult"],
                    "trade_frequency_penalty": params["trade_frequency_penalty"],
                    "turnover_penalty_coef": params["turnover_penalty_coef"], "mode": eval_phase_name,
                    "reward_shaping": False, "warmup_period": warmup_period,
                    "ma5_window": MA5_WINDOW, "ma20_window": MA20_WINDOW, "atr_window": ATR_WINDOW,
                    "rsi_window": RSI_WINDOW, "macd_fast": MACD_FAST, "macd_slow": MACD_SLOW,
                    "macd_signal": MACD_SIGNAL, "momentum_window": MOMENTUM_WINDOW,
                    "cci_window": CCI_WINDOW, "bb_window": BB_WINDOW, "obv_ma_window": OBV_MA_WINDOW,
                }
                final_env_params.update(sim_config)
                final_env_params.update(timing_env_kwargs)
                env = TradingEnv(
                    df,
                    **final_env_params,
                    leak_guard=LeakGuard(LeakConfig(**leak_guard_kwargs)),
                )
                setattr(env, "selected_symbol", symbol)
                env = _wrap_action_space_if_needed(
                    env,
                    bins_vol=bins_vol,
                    action_overrides=action_overrides,
                    long_only=long_only_flag,
                )
                return env

            check_model_compat(str(train_stats_path))
            final_eval_norm = VecNormalize.load(
                str(train_stats_path),
                DummyVecEnv([make_final_eval_env]),
            )
            final_eval_norm.training = False
            final_eval_norm.norm_reward = False
            final_eval_norm.clip_reward = None
            final_eval_env = VecMonitor(final_eval_norm)

            if regime != 'normal':
                final_eval_env.env_method("set_market_regime", regime=regime, duration=regime_duration)

            _rewards, equity_curves = evaluate_policy_custom_cython(model, final_eval_env, num_episodes=1)
            symbol_equity_curves.extend(equity_curves)

            nt_stats = final_eval_env.env_method("get_no_trade_stats")
            if nt_stats:
                total_steps = sum(s["total_steps"] for s in nt_stats if s)
                blocked_steps = sum(s["blocked_steps"] for s in nt_stats if s)
                ratio = blocked_steps / total_steps if total_steps else 0.0
                print(
                    f"[{symbol}] No-trade blocks: {blocked_steps}/{total_steps} steps ({ratio:.2%})"
                )

            final_eval_env.close()

        if not test_stats_path.exists():
            final_eval_norm.save(str(test_stats_path))
            save_sidecar_metadata(str(test_stats_path), extra={"kind": "vecnorm_stats", "phase": eval_phase_name})

        all_returns = [pd.Series(c).pct_change().dropna().to_numpy() for c in symbol_equity_curves if len(c) > 1]
        flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
        final_metrics[regime] = sortino_ratio(flat_returns)

    # --- –†–ê–°–ß–ï–¢ –ò–¢–û–ì–û–í–û–ô –í–ó–í–ï–®–ï–ù–ù–û–ô –ú–ï–¢–†–ò–ö–ò ---
    main_sortino = final_metrics.get('normal', -1.0)
    choppy_score = final_metrics.get('choppy_flat', -1.0)
    trend_score = final_metrics.get('strong_trend', -1.0)

    main_weight = 0.5
    choppy_weight = 0.3
    trend_weight = 0.2
 
    objective_score = (main_weight * main_sortino + choppy_weight * choppy_score + trend_weight * trend_score)

    if model.logger is not None:
        for regime_name, score in final_metrics.items():
            model.logger.record(f"evaluation/{regime_name}_sortino", score)
        model.logger.record("evaluation/final_objective_score", objective_score)
        model.logger.dump(step=model.num_timesteps)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    trial.set_user_attr("main_sortino", main_sortino)
    trial.set_user_attr("choppy_sortino", choppy_score)
    trial.set_user_attr("trend_sortino", trend_score)
    trial.set_user_attr("final_objective", objective_score)

    
    trial.set_user_attr("final_return", 0.0) # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ 0, —Ç.–∫. –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç —É—Å–ª–æ–∂–Ω–µ–Ω

    print(f"\n[‚úÖ Trial {trial.number}] COMPLETE. Final Weighted Score: {objective_score:.4f}")
    print(f"   Components -> Main Sortino: {main_sortino:.4f}, Choppy: {choppy_score:.4f}, Trend: {trend_score:.4f}\n")

    return objective_score

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="configs/config_train.yaml", help="Path to YAML config")
    parser.add_argument(
        "--regime-config",
        default="configs/market_regimes.json",
        help="Path to market regime parameters",
    )
    parser.add_argument(
        "--offline-config",
        default="configs/offline.yaml",
        help="Path to offline dataset configuration with artefact declarations",
    )
    parser.add_argument(
        "--dataset-split",
        default="val",
        help="Dataset split identifier declared in the offline config",
    )
    parser.add_argument(
        "--liquidity-seasonality",
        default=None,
        help="Override path to liquidity seasonality coefficients (defaults to offline bundle)",
    )
    parser.add_argument(
        "--tensorboard-log-dir",
        default="tensorboard_logs",
        help="Directory where TensorBoard events will be written (use 'none' to disable)",
    )
    args, unknown = parser.parse_known_args()

    raw_tensorboard_dir = (args.tensorboard_log_dir or "").strip()
    if raw_tensorboard_dir.lower() in {"", "none", "null"}:
        tensorboard_log_dir = None
    else:
        tensorboard_log_dir = Path(raw_tensorboard_dir).expanduser()

    os.environ["MARKET_REGIMES_JSON"] = args.regime_config

    split_key = (args.dataset_split or "").strip()
    offline_bundle = None
    offline_payload: Mapping[str, Any] | None = None
    offline_split_overrides: dict[str, list[dict[str, int | None]]] = {}
    if split_key and split_key.lower() not in {"none", "null"}:
        try:
            offline_bundle = resolve_split_bundle(args.offline_config, split_key)
        except FileNotFoundError as exc:
            raise SystemExit(f"Offline config not found: {args.offline_config}") from exc
        except KeyError as exc:
            raise SystemExit(
                f"Dataset split '{split_key}' not found in offline config {args.offline_config}"
            ) from exc
        except ValueError as exc:
            raise SystemExit(f"Failed to resolve offline split '{split_key}': {exc}") from exc
        offline_payload = load_offline_payload(args.offline_config)
        offline_split_overrides = _extract_offline_split_overrides(
            offline_payload, split_key, fallback_split="time"
        )

    seasonality_path = args.liquidity_seasonality
    fees_path: str | None = None
    adv_path: str | None = None
    seasonality_hash: str | None = None
    if offline_bundle is not None:
        if offline_bundle.version:
            print(
                f"Resolved offline dataset split '{offline_bundle.name}' version {offline_bundle.version}"
            )
        else:
            print(f"Resolved offline dataset split '{offline_bundle.name}'")
        seasonality_art = offline_bundle.artifacts.get("seasonality")
        if seasonality_art:
            if seasonality_path is None:
                seasonality_path = seasonality_art.path.as_posix()
            raw_hash = seasonality_art.info.artifact.get("verification_hash")
            if raw_hash:
                seasonality_hash = str(raw_hash)
        fees_art = offline_bundle.artifacts.get("fees")
        if fees_art:
            fees_path = fees_art.path.as_posix()
        adv_art = offline_bundle.artifacts.get("adv")
        if adv_art:
            adv_path = adv_art.path.as_posix()

    if seasonality_path is None:
        seasonality_path = "configs/liquidity_seasonality.json"
    args.liquidity_seasonality = seasonality_path

    if not Path(seasonality_path).exists():
        raise FileNotFoundError(
            f"Liquidity seasonality file not found: {seasonality_path}. Run offline builders first."
        )

    cfg = load_config(args.config)

    overrides = {}
    i = 0
    while i < len(unknown):
        if unknown[i].startswith("--") and "." in unknown[i]:
            key = unknown[i][2:]
            if i + 1 >= len(unknown):
                raise ValueError(f"Missing value for argument {unknown[i]}")
            block, field = key.split('.', 1)
            value = yaml.safe_load(unknown[i + 1])
            overrides.setdefault(block, {})[field] = value
            i += 2
        else:
            i += 1

    cfg_dict = cfg.dict()
    if offline_bundle is not None:
        data_block = cfg_dict.setdefault("data", {})
        if offline_bundle.version:
            data_block["split_version"] = offline_bundle.version
        if offline_split_overrides and not data_block.get("split_overrides"):
            data_block["split_overrides"] = offline_split_overrides
        if adv_path:
            adv_block = cfg_dict.setdefault("adv", {})
            if not adv_block.get("path"):
                adv_block["path"] = adv_path
        if fees_path:
            fees_block = cfg_dict.setdefault("fees", {})
            if not fees_block.get("path"):
                fees_block["path"] = fees_path
    for block, params in overrides.items():
        cfg_dict.setdefault(block, {}).update(params)
    cfg_dict["liquidity_seasonality_path"] = args.liquidity_seasonality
    cfg_dict["latency_seasonality_path"] = args.liquidity_seasonality
    latency_block = cfg_dict.setdefault("latency", {})
    if not latency_block.get("latency_seasonality_path"):
        latency_block["latency_seasonality_path"] = args.liquidity_seasonality
    if seasonality_hash:
        cfg_dict["liquidity_seasonality_hash"] = seasonality_hash
    cfg = cfg.__class__.parse_obj(cfg_dict)

    bins_vol = 101
    try:
        maybe = None
        algo_cfg = getattr(cfg, "algo", None)
        if algo_cfg is not None and hasattr(algo_cfg, "action_wrapper"):
            aw = algo_cfg.action_wrapper
            maybe = getattr(aw, "bins_vol", None)
            if maybe is None and hasattr(aw, "__dict__"):
                maybe = aw.__dict__.get("bins_vol")
        if maybe is None and hasattr(cfg, "__dict__"):
            maybe = (cfg.__dict__.get("algo", {}) or {}).get("action_wrapper", {}).get("bins_vol")
        if maybe is not None:
            bins_vol = int(maybe)
    except Exception:
        bins_vol = 101
    try:
        bins_vol = max(2, int(bins_vol))
    except Exception:
        bins_vol = 101

    timing_defaults, timing_profiles = load_timing_profiles()
    exec_profile = getattr(cfg, "execution_profile", ExecutionProfile.MKT_OPEN_NEXT_H1)
    resolved_timing = resolve_execution_timing(exec_profile, timing_defaults, timing_profiles)
    timing_env_kwargs = {
        "decision_mode": DecisionTiming[resolved_timing.decision_mode],
        "decision_delay_ms": resolved_timing.decision_delay_ms,
        "latency_steps": resolved_timing.latency_steps,
    }
    leak_guard_kwargs = {
        "decision_delay_ms": resolved_timing.decision_delay_ms,
        "min_lookback_ms": resolved_timing.min_lookback_ms,
    }

    sim_config = {k: getattr(cfg, k) for k in ("quantizer", "slippage", "fees", "latency", "risk", "no_trade")}
    sim_config["liquidity_seasonality_path"] = args.liquidity_seasonality
    liq_hash = _file_sha256(args.liquidity_seasonality)
    if liq_hash:
        sim_config["liquidity_seasonality_hash"] = liq_hash
        print(f"Liquidity seasonality hash: {liq_hash}")
    else:
        print(f"Warning: could not compute hash for {args.liquidity_seasonality}; seasonality consistency not enforced")

    processed_data_dir = getattr(cfg.data, "processed_dir", "data/processed")
    run_id = getattr(cfg, "run_id", "default-run")
    artifacts_root = Path(getattr(cfg, "artifacts_dir", "models")) / run_id
    trials_dir = artifacts_root / "trials"
    N_ENSEMBLE = int(cfg.model.params.get("n_ensemble", 5))

    if os.path.exists(trials_dir):
        print(f"–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ '{trials_dir}'...")
        shutil.rmtree(trials_dir)
    os.makedirs(trials_dir, exist_ok=True)

    print("Loading all pre-processed data...")
    all_feather_files = glob.glob(os.path.join(processed_data_dir, "*.feather"))
    if not all_feather_files:
        raise FileNotFoundError(
            f"No .feather files found in {processed_data_dir}. "
            f"Run prepare_advanced_data.py (Fear&Greed), prepare_events.py (macro events), "
            f"incremental_klines.py (1h candles), then prepare_and_run.py (merge/export).",
        )
    all_dfs_dict, all_obs_dict = load_all_data(all_feather_files, synthetic_fraction=0, seed=42)

    split_version, time_splits = _load_time_splits(cfg.data)
    if split_version:
        print(f"Using split version: {split_version}")
    timestamp_column = getattr(cfg.data, "timestamp_column", "timestamp")
    role_column = getattr(cfg.data, "role_column", "wf_role")

    dfs_with_roles: dict[str, pd.DataFrame] = {}
    inferred_test_any = False
    for symbol, df in all_dfs_dict.items():
        annotated, inferred_flag = _apply_role_column(df, time_splits, timestamp_column, role_column)
        dfs_with_roles[symbol] = annotated
        inferred_test_any = inferred_test_any or inferred_flag

    _ensure_validation_split_present(dfs_with_roles, time_splits, timestamp_column, role_column)

    train_intervals = time_splits.get("train", [])
    train_start_candidates = [start for start, _ in train_intervals if start is not None]
    train_end_candidates = [end for _, end in train_intervals if end is not None]
    train_start_ts = min(train_start_candidates) if train_start_candidates else None
    train_end_ts = max(train_end_candidates) if train_end_candidates else None

    PREPROC_PATH = artifacts_root / "preproc_pipeline.json"
    pipe = FeaturePipeline()
    pipe.fit(
        dfs_with_roles,
        train_mask_column=role_column,
        train_mask_values={"train"},
        train_start_ts=train_start_ts,
        train_end_ts=train_end_ts,
        timestamp_column=timestamp_column,
        split_version=split_version,
        train_intervals=train_intervals,
    )
    PREPROC_PATH.parent.mkdir(parents=True, exist_ok=True)
    pipe.save(str(PREPROC_PATH))
    try:
        save_sidecar_metadata(
            str(PREPROC_PATH),
            extra={
                "kind": "feature_pipeline",
                "split_version": split_version,
                "train_start_ts": train_start_ts,
                "train_end_ts": train_end_ts,
            },
        )
    except Exception:
        # Sidecar metadata helper may not be available in all environments.
        pass
    all_dfs_with_roles = pipe.transform_dict(dfs_with_roles, add_suffix="_z")
    print(f"Feature pipeline fitted and saved to {PREPROC_PATH}. Standardized columns *_z added.")
    print("To run inference over processed data, execute: python infer_signals.py")

    # --- –ì–µ–π—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: —Å—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è OHLCV –ø–µ—Ä–µ–¥ —Å–ø–ª–∏—Ç–æ–º ---
    _validator = DataValidator()
    for _key, _df in all_dfs_with_roles.items():
        try:
            # frequency=None -> –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–∏ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
            _validator.validate(_df, frequency=None)
        except Exception as e:
            raise RuntimeError(f"Data validation failed for asset '{_key}': {e}")
    print("‚úì Data validation passed for all assets.")

    def _extract_phase(phase: str) -> dict[str, pd.DataFrame]:
        out: dict[str, pd.DataFrame] = {}
        for sym, df in all_dfs_with_roles.items():
            mask = df[role_column].astype(str) == phase
            phase_df = df.loc[mask].copy()
            if not phase_df.empty:
                out[sym] = phase_df.reset_index(drop=True)
        return out

    train_data_by_token = _extract_phase("train")
    val_data_by_token = _extract_phase("val")
    test_data_by_token = _extract_phase("test")

    def _select_obs(mapping: dict[str, pd.DataFrame]) -> dict[str, np.ndarray]:
        return {sym: all_obs_dict[sym] for sym in mapping if sym in all_obs_dict}

    train_obs_by_token = _select_obs(train_data_by_token)
    val_obs_by_token = _select_obs(val_data_by_token)
    test_obs_by_token = _select_obs(test_data_by_token)

    unused_rows = {
        sym: int((df[role_column].astype(str) == "none").sum())
        for sym, df in all_dfs_with_roles.items()
        if (df[role_column].astype(str) == "none").any()
    }
    if unused_rows:
        total_unused = sum(unused_rows.values())
        print(
            f"Warning: {total_unused} rows across {len(unused_rows)} symbols were not assigned to train/val/test and will be ignored."
        )

    print("Time-based split summary:")
    for phase, mapping in (
        ("train", train_data_by_token),
        ("val", val_data_by_token),
        ("test", test_data_by_token),
    ):
        intervals = time_splits.get(phase, [])
        interval_desc = ", ".join(_format_interval(it) for it in intervals) if intervals else "(inferred remainder)"
        total_rows = sum(len(df) for df in mapping.values())
        observed_start, observed_end = _phase_bounds(mapping, timestamp_column)
        print(
            f"  {phase}: {len(mapping)} symbols, {total_rows} rows, intervals={interval_desc}, "
            f"observed=[{_fmt_ts(observed_start)} .. {_fmt_ts(observed_end)}]"
        )
    if inferred_test_any and not time_splits.get("test"):
        print("  Note: test split inferred from remaining rows (no explicit interval provided).")

    print("Calculating per-asset normalization stats from the training set...")
    norm_stats = {}

    # –ò—Ç–µ—Ä–∏—Ä—É–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É –∞–∫—Ç–∏–≤—É –≤ –¢–†–ï–ù–ò–†–û–í–û–ß–ù–û–ú –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    for asset_key, train_df in train_data_by_token.items():
        
        # 1. –ù–∞—Ö–æ–¥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –î–ê–ù–ù–û–ú –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –∞–∫—Ç–∏–≤–µ
        features_to_normalize = [
            col for col in train_df.columns 
            if '_norm' in col and col not in ['log_volume_norm', 'fear_greed_value_norm']
        ]
        
        if not features_to_normalize:
            continue # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É —ç—Ç–æ–≥–æ –∞—Å—Å–µ—Ç–∞ –Ω–µ—Ç —Ç–∞–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            
        # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¢–û–õ–¨–ö–û –ø–æ –¥–∞–Ω–Ω—ã–º —ç—Ç–æ–≥–æ –∞—Å—Å–µ—Ç–∞
        mean_stats = train_df[features_to_normalize].mean().to_dict()
        std_stats = train_df[features_to_normalize].std().to_dict()
        
        # 3. –ù–∞—Ö–æ–¥–∏–º ID —Ç–æ–∫–µ–Ω–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å —ç—Ç–∏–º –∞—Å—Å–µ—Ç–æ–º
        # (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –æ–¥–∏–Ω —Ñ–∞–π–ª = –æ–¥–∏–Ω –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–æ–∫–µ–Ω)
        if 'token_id' in train_df.columns:
            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            if not train_df.empty:
                token_id = train_df['token_id'].iloc[0]
                
                # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
                norm_stats[str(token_id)] = {'mean': mean_stats, 'std': std_stats}

    norm_stats_path = artifacts_root / "norm_stats.json"
    with open(norm_stats_path, "w") as f:
        json.dump(norm_stats, f, indent=4)
    print(f"Per-asset normalization stats for {len(norm_stats)} tokens calculated and saved.")

    HPO_TRIALS = 20 # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π
    HPO_BUDGET_PER_TRIAL = 1_000_000 # –¢–∞–π–º—Å—Ç–µ–ø—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏—Å–ø—ã—Ç–∞–Ω–∏—è

    print(f"\n===== Starting Unified HPO Process ({HPO_TRIALS} trials) =====")

    sampler = TPESampler(n_startup_trials=5, seed=42)
    pruner = HyperbandPruner()
    study = optuna.create_study(direction="maximize", sampler=sampler, pruner=pruner)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –Ω–∞ –ü–û–õ–ù–û–ú, –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
    study.optimize(
        lambda t: objective(
            t,
            cfg,
            HPO_BUDGET_PER_TRIAL,
            train_data_by_token,
            train_obs_by_token,
            val_data_by_token,
            val_obs_by_token,
            test_data_by_token,
            test_obs_by_token,
            norm_stats,
            sim_config,
            timing_env_kwargs,
            leak_guard_kwargs,
            trials_dir,
            tensorboard_log_dir,
        ),
        n_trials=HPO_TRIALS,
        n_jobs=1,
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
    final_study = study
    if not final_study:
        print("No final study completed. Exiting.")
        return
    # <-- –ö–û–ù–ï–¶ –ë–õ–û–ö–ê –î–õ–Ø –ó–ê–ú–ï–ù–´ -->

    print(f"\nSaving best {N_ENSEMBLE} models from the final stage...")
    ensemble_dir = artifacts_root / "ensemble"
    if os.path.exists(ensemble_dir):
        shutil.rmtree(ensemble_dir)
    os.makedirs(ensemble_dir)

    top_trials = sorted(final_study.trials, key=lambda tr: tr.value or -1e9, reverse=True)[:N_ENSEMBLE]

    ensemble_meta = []
    for i, trial in enumerate(top_trials):
        model_idx = i + 1
        src_model = trials_dir / f"trial_{trial.number}_model.zip"
        src_stats = trials_dir / f"vec_normalize_{trial.number}.pkl"

        if os.path.exists(src_model):
            shutil.copyfile(src_model, ensemble_dir / f"model_{model_idx}.zip")
            if os.path.exists(src_stats):
                shutil.copyfile(src_stats, ensemble_dir / f"vec_normalize_{model_idx}.pkl")

            ensemble_meta.append({"ensemble_index": model_idx, "trial_number": trial.number, "value": trial.value, "params": trial.params})
        else:
            print(f"‚ö†Ô∏è WARNING: Could not find model for trial {trial.number}. Skipping.")
    # –ö–æ–ø–∏—Ä—É–µ–º –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π,
    # —Ç–∞–∫ –∫–∞–∫ –æ–Ω —è–≤–ª—è–µ—Ç—Å—è –Ω–µ–æ—Ç—ä–µ–º–ª–µ–º–æ–π —á–∞—Å—Ç—å—é –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ.
    src_norm_stats = artifacts_root / "norm_stats.json"
    if os.path.exists(src_norm_stats):
        shutil.copyfile(src_norm_stats, ensemble_dir / "norm_stats.json")
    else:
        print(f"‚ö†Ô∏è CRITICAL WARNING: Could not find the global 'norm_stats.json' file. The saved ensemble will not be usable for inference.")
    with open(ensemble_dir / "ensemble_meta.json", "w") as f:
        json.dump(ensemble_meta, f, indent=4)
    print(f"\n‚úÖ Ensemble of {len(ensemble_meta)} models saved to '{ensemble_dir}'. HPO complete.")

    # --- Validation of the best model for reproducibility ---
    best_model_path = ensemble_dir / "model_1.zip"
    best_stats_path = ensemble_dir / "vec_normalize_1.pkl"
    if best_model_path.exists() and best_stats_path.exists():
        print("\nRunning validation on the best ensemble model...")
        best_trial = top_trials[0]

        final_eval_data = test_data_by_token if test_data_by_token else val_data_by_token
        final_eval_obs = test_obs_by_token if test_data_by_token else val_obs_by_token
        final_eval_mode = "test" if test_data_by_token else "val"
        if not final_eval_data:
            print("‚ö†Ô∏è Skipping final validation: evaluation split is empty.")
        else:
            def _make_env_val(symbol: str, df: pd.DataFrame):
                params = best_trial.params
                env_val_params = {
                    "norm_stats": norm_stats,
                    "window_size": params["window_size"],
                    "gamma": params["gamma"],
                    "atr_multiplier": params["atr_multiplier"],
                    "trailing_atr_mult": params["trailing_atr_mult"],
                    "tp_atr_mult": params["tp_atr_mult"],
                    "trade_frequency_penalty": params["trade_frequency_penalty"],
                    "turnover_penalty_coef": params["turnover_penalty_coef"],
                    "mode": final_eval_mode,
                    "reward_shaping": False,
                    "warmup_period": warmup_period,
                    "ma5_window": MA5_WINDOW,
                    "ma20_window": MA20_WINDOW,
                    "atr_window": ATR_WINDOW,
                    "rsi_window": RSI_WINDOW,
                    "macd_fast": MACD_FAST,
                    "macd_slow": MACD_SLOW,
                    "macd_signal": MACD_SIGNAL,
                    "momentum_window": MOMENTUM_WINDOW,
                    "cci_window": CCI_WINDOW,
                    "bb_window": BB_WINDOW,
                    "obv_ma_window": OBV_MA_WINDOW,
                }
                env_val_params.update(sim_config)
                env_val_params.update(timing_env_kwargs)
                env = TradingEnv(
                    df,
                    **env_val_params,
                    leak_guard=LeakGuard(LeakConfig(**leak_guard_kwargs))
                )
                setattr(env, "selected_symbol", symbol)
                env = _wrap_action_space_if_needed(
                    env,
                    bins_vol=bins_vol,
                    action_overrides=action_overrides,
                    long_only=long_only_flag,
                )
                return env

            eval_env_fns: list[Callable[[], TradingEnv]] = []
            for symbol, df in sorted(final_eval_data.items()):
                def _factory(symbol=symbol, df=df):
                    return _make_env_val(symbol, df)

                eval_env_fns.append(_factory)

            monitored_eval_env = VecMonitor(DummyVecEnv(eval_env_fns))
            check_model_compat(str(best_stats_path))
            eval_env = VecNormalize.load(str(best_stats_path), monitored_eval_env)
            eval_env.training = False
            eval_env.norm_reward = False
            eval_env.clip_reward = None

            DistributionalPPO = _get_distributional_ppo()
            best_model = DistributionalPPO.load(str(best_model_path), env=eval_env)

            rewards, equity_curves = evaluate_policy_custom_cython(
                best_model, eval_env, num_episodes=max(1, len(final_eval_data))
            )
            all_returns = [
                pd.Series(curve).pct_change().dropna().to_numpy()
                for curve in equity_curves if len(curve) > 1
            ]
            flat_returns = np.concatenate(all_returns) if all_returns else np.array([0.0])
            sortino = sortino_ratio(flat_returns)
            sharpe = sharpe_ratio(flat_returns)

            report = {
                "mean_reward": float(np.mean(rewards)),
                "std_reward": float(np.std(rewards)),
                "sortino_ratio": float(sortino),
                "sharpe_ratio": float(sharpe),
            }
            with open(ensemble_dir / "validation_report.json", "w") as f:
                json.dump(report, f, indent=4)
            print(
                f"Validation metrics -> Sortino: {sortino:.4f}, Sharpe: {sharpe:.4f}. "
                f"Report saved to '{ensemble_dir / 'validation_report.json'}'"
            )
    else:
        print(
            "‚ö†Ô∏è Could not find best model or normalization stats for validation evaluation."
        )

if __name__ == "__main__":
    try:
        mp.set_start_method("spawn", force=True)
    except RuntimeError:
        pass

    def _extract_grad_sanity(argv: list[str]) -> str | None:
        for idx, arg in enumerate(argv):
            if arg == "--grad-sanity":
                if idx + 1 < len(argv) and not argv[idx + 1].startswith("--"):
                    return argv[idx + 1]
                return "1"
            if arg.startswith("--grad-sanity="):
                value = arg.split("=", 1)[1]
                return value if value else "1"
        return None

    flag_value = _extract_grad_sanity(sys.argv[1:])
    if flag_value is not None:
        os.environ["GRAD_SANITY"] = flag_value

    # --- gradient sanity check (–≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ñ–ª–∞–≥–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏—è) ---
    from runtime_flags import get_bool
    if get_bool("GRAD_SANITY", False):
        from tools.grad_sanity import run_check
        run_check()

    main()
